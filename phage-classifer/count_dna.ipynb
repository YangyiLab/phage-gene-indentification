{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import phanotate_modules.functions as phano\n",
    "import numpy as np\n",
    "import pyfastx\n",
    "import phanotate_modules.functions as phano\n",
    "shotsetds=[]\n",
    "shotsetss=[]\n",
    "\n",
    "fa_ds = pyfastx.Fasta('ds-linear.fasta')\n",
    "fa_ss = pyfastx.Fasta('ss-circular.fasta')\n",
    "fa_ds_1 = pyfastx.Fasta('ds-circular.fasta')\n",
    "fa_ss_1 = pyfastx.Fasta('ss-linear.fasta')\n",
    "def shot(str):\n",
    "    length=len(str)\n",
    "    start=np.random.randint(0,length,10)\n",
    "    shot_len=np.random.normal(3000,200,1)\n",
    "    return [str[s:int(s+shot_len)] for s in start]\n",
    "\n",
    "for itm in fa_ds:\n",
    "    for _ in range(1):\n",
    "        shotsetds=shotsetds+shot(itm.seq)\n",
    "\n",
    "for itm in fa_ds_1:\n",
    "    for _ in range(1):\n",
    "        shotsetds=shotsetds+shot(itm.seq)\n",
    "\n",
    "for itm in fa_ss_1:\n",
    "    for _ in range(1):\n",
    "        shotsetss=shotsetss+shot(itm.seq)\n",
    "\n",
    "for itm in fa_ss:\n",
    "    for _ in range(1):\n",
    "        shotsetss=shotsetss+shot(itm.seq)\n",
    "shotsetds=[phano.get_backgroud_rbs(i) for i in shotsetds]\n",
    "# shotsetds = torch.Tensor(shotsetds)\n",
    "# shotsetds = torch.Tensor.reshape(shotsetds,(-1,28))\n",
    "shotsetss=[phano.get_backgroud_rbs(i) for i in shotsetss]\n",
    "# shotsetss = torch.Tensor(shotsetss)\n",
    "# shotsetss = torch.Tensor.reshape(shotsetss,(-1,28))\n",
    "shotsetlist=shotsetds+shotsetss\n",
    "shotsetlist=np.array(shotsetlist,dtype=np.float32)\n",
    "labels1=np.zeros(len(fa_ds)*10)\n",
    "labels2=np.zeros(len(fa_ds_1)*10)+1\n",
    "labels3=np.zeros(len(fa_ss_1)*10)+2\n",
    "labels4=np.zeros(len(fa_ss)*10)+3\n",
    "# label=[labels1,labels2,labels3,labels4]\n",
    "label=np.r_[labels1,labels2,labels3,labels4]\n",
    "label=np.array(label,dtype=np.int)\n",
    "\n",
    "filename=\"result.txt\"\n",
    "filename= save_variable(shotsetlist,filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Two-Class label"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shotsetlist=load_variavle(\"result.txt\")\n",
    "labels1=np.zeros(len(fa_ds)*10+len(fa_ds_1)*10)\n",
    "labels2=np.zeros(len(fa_ss_1)*10+len(fa_ss)*10)+1\n",
    "label=np.r_[labels1,labels2]\n",
    "label=np.array(label,dtype=np.int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels1=np.zeros(len(fa_ds)*10)\n",
    "labels2=np.zeros(len(fa_ds_1)*10)+1\n",
    "labels3=np.zeros(len(fa_ss_1)*10)+2\n",
    "labels4=np.zeros(len(fa_ss)*10)+3\n",
    "label=np.r_[labels1,labels2,labels3,labels4]\n",
    "label=np.array(label,dtype=np.int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    "def load_variavle(filename):\n",
    "   f=open(filename,'rb')\n",
    "   r=pickle.load(f)\n",
    "   f.close()\n",
    "   return r\n",
    "\n",
    "shotsetlist=load_variavle(\"result.txt\")\n",
    "labels1=np.zeros(len(fa_ds)*10+len(fa_ds_1)*10)\n",
    "# labels2=np.zeros(len(fa_ds_1)*10)+1\n",
    "labels2=np.zeros(len(fa_ss_1)*10+len(fa_ss)*10)+1\n",
    "# labels4=np.zeros(len(fa_ss)*10)+3\n",
    "# label=[labels1,labels2,labels3,labels4]\n",
    "label=np.r_[labels1,labels2]\n",
    "label=np.array(label,dtype=np.int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#x为数据集的feature熟悉，y为label.\n",
    "x_train, x_test, y_train, y_test = train_test_split(shotsetlist[0:len(labels1)+len(labels2)], label[0:len(labels1)+len(labels2)], test_size = 0.2,random_state=4)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(shotsetlist)\n",
    "shotsetlist=scaler.transform(shotsetlist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNN method "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "model = nn.Sequential(nn.Linear(28,16),nn.Linear(16,128),nn.ReLU(),nn.Linear(128,2))\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Loss and optimizer\n",
    "# weight=torch.from_numpy(np.array([0.1,1.0,1.0,1.0],dtype=np.float32))\n",
    "loss = nn.CrossEntropyLoss(size_average=True)\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=1)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "# d2l.train_ch3(model, x_train, y_train, loss, num_epochs, trainer)\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "class CustomDataset(data.Dataset):#需要继承data.Dataset\n",
    "    def __init__(self,x,y):\n",
    "        # TODO\n",
    "        # 1. Initialize file path or list of file names.\n",
    "        self.X=x \n",
    "        self.y=y\n",
    "        self.length=len(self.X)\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        return self.X[index],self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return self.length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shotset=CustomDataset(torch.Tensor(x_train),y_train)\n",
    "myloader=data.DataLoader(shotset,batch_size=256)\n",
    "testset=CustomDataset(torch.Tensor(x_test),y_test)\n",
    "testloader=data.DataLoader(shotset,batch_size=32 )\n",
    "d2l.train_ch3(model,myloader,testloader,loss,100,trainer)\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=0.1)  \n",
    "d2l.train_ch3(model,myloader,testloader,loss,100,trainer)\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制\n",
    "res=softmax(model(torch.Tensor(x_test)))\n",
    "res=softmax(model(torch.Tensor(x_test)))\n",
    "filtered_idx=[]\n",
    "number=0\n",
    "right_num=0\n",
    "for i in range(len(res)):\n",
    "    idx=res[i].argmax(0)\n",
    "    if(res[i][idx]>0.71):\n",
    "        filtered_idx.append(i)\n",
    "        number=number+1\n",
    "        if (idx== y_test[i]):\n",
    "            right_num=right_num+1\n",
    "right_num/number , number/len(y_test)\n",
    "train(model,myloader,test_iter=testloader,loss=loss,num_epochs=1000,trainer=trainer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#x为数据集的feature熟悉，y为label.\n",
    "# x_train, x_test, y_train, y_test = train_test_split(dna_data, labels, test_size = 0.2,random_state=4)\n",
    "from sklearn import tree\n",
    "X = x_train\n",
    "Y = y_train\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.max_depth=3\n",
    "clf = clf.fit(X, Y)\n",
    "# scores=cross_val_score(clf,x_test,y_test)\n",
    "# scores.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High Precision Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(max_depth=None, min_samples_leaf=2,random_state=0,bootstrap = True,\n",
    "n_estimators=200,oob_score=True)\n",
    "clf = clf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "number=0\n",
    "right_num=0\n",
    "index=[]\n",
    "for i in range(len(x_test)):\n",
    "    result=clf.predict_proba(x_test[i:i+1])\n",
    "    pred=clf.predict(x_test[i:i+1])\n",
    "    if np.max(result)>0.45:\n",
    "        number=number+1\n",
    "        index.append(i)\n",
    "        if pred[0]==y_test[i]:\n",
    "            right_num=right_num+1\n",
    "        \n",
    "right_num/number"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ada Boost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "clf = AdaBoostClassifier(n_estimators=150,base_estimator=tree.DecisionTreeClassifier())\n",
    "clf = clf.fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(x_train,y_train)\n",
    "scores=cross_val_score(gnb,x_test,y_test)\n",
    "scores.mean()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Analysis\n",
    "### AUC score and map for 2-class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
    "fpr, tpr, thresholds= roc_curve(y_test,clf.predict(x_test))\n",
    "from matplotlib import pyplot as plt\n",
    "auc(fpr, tpr)\n",
    "roc_auc_score(y_test,clf.predict(x_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "index=[i for i in range(len(x_test)) if i in filtered_idx ]\n",
    "y_true=y_train\n",
    "# y_pred=clf.predict(x_train)\n",
    "C=confusion_matrix(y_test[index], model(torch.Tensor(x_test[index])).argmax(1))\n",
    "index=[\"ds-linear\", \"ds-circular\",\"ss-linear\",\"ss-circular\"]\n",
    "df=pd.DataFrame(C,index=index[0:2],\n",
    "columns=index[0:2])\n",
    "sns.heatmap(df,annot=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}